# -*- coding: utf-8 -*-
"""BlackCoffer_Assignement.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DpsXRJJGRqRoAuqc9hslTBdFJI9VFDYu
"""

#importing library
import pandas as pd
import requests
from bs4 import BeautifulSoup
import numpy as np
from google.colab import files
import nltk
import string
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

#importing input file
#df=pd.read_csv("/content/Input.csv.csv")[['URL_ID','URL']]

url_data = pd.read_excel("/content/drive/MyDrive/BlackCoffer_Assignment2023/20211030 Test Assignment/Input.xlsx")

url_data.head()
url_data.shape
url_data["URL"][0]

df = pd.DataFrame({"URL":[''],"Title":[''],"Content":['']})

for i in range(114):
    page = requests.get(url_data["URL"][i],headers={"User-Agent": "XY"})

    soup = BeautifulSoup(page.text,"lxml")
    try:
        title = soup.find("h1",class_="entry-title").text
    except:
        None
    try:
        content = soup.find("div",class_="td-post-content").text.replace("\n",'')
    except:
        None
    df = df.append({"URL":url_data["URL"][i],"Title":title,"Content":content},ignore_index=True)

df.head()





df = df.iloc[1:]
df.shape

df

df['Content'] = df['Content'].apply(lambda x:x.lower())

stop_words_list = pd.read_csv('/content/drive/MyDrive/BlackCoffer_Assignment2023/20211030 Test Assignment/StopWords/StopWords_Generic.txt')

my_stopwords = stop_words_list.values.tolist()

import nltk


nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
library_stopwords = nltk.corpus.stopwords.words('english')
library_stopwords.extend(my_stopwords)

df['Content'] = df['Content'].apply(lambda x: " ".join([i for i in x.split() if i not in library_stopwords]))

df['Content']

import itertools
positive_words = pd.read_csv('/content/drive/MyDrive/BlackCoffer_Assignment2023/20211030 Test Assignment/MasterDictionary/positive-words.txt')
my_postive_words = positive_words.values.tolist()
my_positive_words_modified = list(itertools.chain.from_iterable(my_postive_words))

df['Positive_Score'] = df['Content'].apply(lambda x: len([x for x in x.split() if x in my_positive_words_modified]))

df.head()

import itertools
negative_words = pd.read_csv('/content/drive/MyDrive/BlackCoffer_Assignment2023/20211030 Test Assignment/MasterDictionary/negative-words.txt',encoding='latin-1')
my_negative_words = negative_words.values.tolist()
my_negative_words_modified = list(itertools.chain.from_iterable(my_negative_words))

df['Negative_Score'] = df['Content'].apply(lambda x: len([x for x in x.split() if x in my_negative_words_modified]))

df.head()

for i in range(len(df)):
    df['Polarity_Score'] = (df['Positive_Score'] - df['Negative_Score'])/((df['Positive_Score'] + df['Negative_Score'])
                                                                          + 0.000001)

df.head()

for i in range(len(df)):
    df['Subjectivity_Score'] = (df['Positive_Score'] + df['Negative_Score']) / (len(df['Content']) + 0.000001)
df.head()

def avg_sentence_len(text):
    sentences = text.split(".")
    words = text.split(" ")
    if(sentences[len(sentences)-1]==""):
        average_sentence_length = len(words) / len(sentences)-1
    else:
        average_sentence_length = len(words) / len(sentences)
    return average_sentence_length

df['Average_Sentence_Length'] = df['Content'].apply(lambda x: avg_sentence_len(x))

pip install syllapy

import syllapy
count = 0
complex_words = []
for i in range(len(df)):
    text = df['Content'][i+1].split()
    for j in range(len(text)):
        if syllapy.count(text[j]) > 2:
            count = count + 1
    complex_words.append(count)

complex_length = []
for words in range(len(df)):
    length = (complex_words[words])/(len(df['Content'][words + 1]))
    complex_length.append(length)

df['Percentage_of_Complex_Words'] = complex_length

index_score = []
for score in range(len(df)):
    value = 0.4*((df['Average_Sentence_Length'][score+1]) + (df['Percentage_of_Complex_Words'][score+1]))
    index_score.append(value)
df['Fog_Index'] = index_score

import re
average_words = []
for i in range(len(df)):
    parts = [len(l.split()) for l in re.split(r'[?!.]', df['Content'][i+1]) if l.strip()]
    words = sum(parts)/len(parts)
    average_words.append(words)

df['Average_Number_Of_Words_per_Sentence'] = average_words

df['Complex_Words'] = complex_words

import string
def punctuation_remove(text_data):
    clean_data = ''.join([i for i in text_data if i not in string.punctuation])
    return clean_data
df['Content'] = df['Content'].apply(lambda x: punctuation_remove(x))

words = []
for word in range(len(df)):
    content_length = len(df['Content'][word+1])
    words.append(content_length)
df['Word_Count'] = words

vowels = ['a','e','i','o','u']
syllable = 0
syllable_count = []
for websites in range(len(df)):
    for post in df['Content'][websites+1]:
        for letter in post:
            if letter in vowels:
                syllable = syllable + 1

    syllable_count.append(syllable)
df['Syllable_Count_'] = syllable_count

pronoun_count = []
for websites in range(len(df)):
    pronounRegex = re.compile(r'\b(I|we|my|ours|(?-i:us))\b',re.I)
    pronoun = pronounRegex.findall(df['Content'][websites+1])
    pronoun_length = len(pronoun)
    pronoun_count.append(pronoun_length)
df['Personal_Pronouns'] = pronoun_count

average_word_length = []
for websites in range(len(df)):
    for post in df['Content'][websites+1]:
        total_characters = len(post)
        total_words = len(post.split(" "))
        average = total_characters/total_words
    average_word_length.append(average)
df['Average_Word_Length'] = average_word_length

df.head(2)

df.to_csv("C:\\Users\\PRANESH\\Downloads\\Output Data Structure.xlsx - Sheet1.csv")

from google.colab import drive
drive.mount('/content/drive')

df.to_csv("/content/drive/MyDrive/result.csv", encoding='utf-8',index=False)

df.drop('Title',axis=1,inplace=True)

df.drop('Content',axis=1,inplace=True)

df.head(2)